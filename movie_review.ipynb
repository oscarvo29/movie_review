{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/oscarotterstad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/oscarotterstad/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oscarotterstad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  bromwel high cartoon comedi ran time program s...      1\n",
      "1  homeless houseless georg carlin state issu yea...      1\n",
      "2  brilliant overact lesley ann warren best drama...      1\n",
      "3  easili underr film inn brook cannon sure flaw ...      1\n",
      "4  typic mel brook film much less slapstick movi ...      1\n",
      "Best model parameters: {'classifier__alpha': 1.0}\n",
      "Model saved to movie_review_model.joblib\n",
      "Message: The movie did not entertain me. I barely felt any joy watching this.\n",
      "Prediction: Positive\n",
      "Positive Probability: 0.66\n",
      "Negative Probability: 0.34\n",
      "--------------------------------------------------\n",
      "Message: Star Wars always get a kick out of me. I really loved this movie.\n",
      "Prediction: Positive\n",
      "Positive Probability: 1.00\n",
      "Negative Probability: 0.00\n",
      "--------------------------------------------------\n",
      "Message: Super boring movie.\n",
      "Prediction: Negative\n",
      "Positive Probability: 0.01\n",
      "Negative Probability: 0.99\n",
      "--------------------------------------------------\n",
      "Message: Classy, elegant, joyful. Home Alone always get me in the christmas spirit.\n",
      "Prediction: Positive\n",
      "Positive Probability: 0.98\n",
      "Negative Probability: 0.02\n",
      "--------------------------------------------------\n",
      "Message: The CGI in The Hobit was disgusting! How could they go from Lord of The Rings to this?\n",
      "Prediction: Negative\n",
      "Positive Probability: 0.04\n",
      "Negative Probability: 0.96\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "\n",
    "# df = pd.read_json(\"skills_assessment_data/train.json\")\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def process_data(data: pd.DataFrame) -> pd.DataFrame: \n",
    "    # Remove duplicates if any\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "    data[\"text\"] = data[\"text\"].str.lower()\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: re.sub(r\"[^a-z\\s$!]\", \"\", x))\n",
    "    data[\"text\"] = data[\"text\"].apply(word_tokenize)\n",
    "\n",
    "    # Define a set of English stop words and remove them from the tokens\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: \" \".join(x))\n",
    "    data[\"label\"] = data[\"label\"].apply(lambda x: 0 if x == 1 else 1)\n",
    "    return data \n",
    "\n",
    "df = process_data(pd.read_json(\"skills_assessment_data/train.json\"))\n",
    "\n",
    "# Initialize CountVectorizer with bigrams, min_df, and max_df to focus on relevant terms\n",
    "vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the message column\n",
    "X = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "y = df[\"label\"] \n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", MultinomialNB())\n",
    "])\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"classifier__alpha\": [0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0]\n",
    "}\n",
    "\n",
    "# # Perform the grid search with 5-fold cross-validation and the F1-score as metric\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1\"\n",
    ")\n",
    "\n",
    "# # Fit the grid search on the full dataset\n",
    "grid_search.fit(df[\"text\"], y)\n",
    "\n",
    "# Extract the best model identified by the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best model parameters:\", grid_search.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "# Save the trained model to a file for future use\n",
    "model_filename = 'movie_review_model.joblib'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "\n",
    "new_reviews = [\n",
    "    \"The movie did not entertain me. I barely felt any joy watching this.\",\n",
    "    \"Star Wars always get a kick out of me. I really loved this movie.\",\n",
    "    \"Super boring movie.\",\n",
    "    \"Classy, elegant, joyful. Home Alone always get me in the christmas spirit.\",\n",
    "    \"The CGI in The Hobit was disgusting! How could they go from Lord of The Rings to this?\",\n",
    "]\n",
    "\n",
    "# Preprocess function that mirrors the training-time preprocessing\n",
    "def preprocess_reviews(review):\n",
    "    review = review.lower()\n",
    "    review = re.sub(r\"[^a-z\\s$!]\", \"\", review)\n",
    "    tokens = word_tokenize(review)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "processed_reviews = [preprocess_reviews(msg) for msg in new_reviews]\n",
    "\n",
    "X_new = best_model.named_steps[\"vectorizer\"].transform(processed_reviews)\n",
    "# Predict with the trained classifier\n",
    "predictions = best_model.named_steps[\"classifier\"].predict(X_new)\n",
    "prediction_probabilities = best_model.named_steps[\"classifier\"].predict_proba(X_new)\n",
    "\n",
    "# Display predictions and probabilities for each evaluated message\n",
    "for i, msg in enumerate(new_reviews):\n",
    "    prediction = \"Negative\" if predictions[i] == 1 else \"Positive\"\n",
    "    postitive_probability = prediction_probabilities[i][0]  # Probability of being spam\n",
    "    negative_probability = prediction_probabilities[i][1]   # Probability of being not spam\n",
    "    \n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Positive Probability: {postitive_probability:.2f}\")\n",
    "    print(f\"Negative Probability: {negative_probability:.2f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8483932099512116\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = joblib.load('movie_review_model.joblib')\n",
    "df_test = process_data(pd.read_json(\"skills_assessment_data/test.json\"))\n",
    "X_test = best_model.named_steps[\"vectorizer\"].transform(df_test[\"text\"])\n",
    "y_test = df_test[\"label\"]\n",
    "# Predict with the trained classifier\n",
    "predictions = best_model.named_steps[\"classifier\"].predict(X_test)\n",
    "prediction_probabilities = best_model.named_steps[\"classifier\"].predict_proba(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
